{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "notes = pd.read_csv(\"notes.csv\")\n",
    "listings = pd.read_csv(\"listings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0.9827\n",
       "1         0.8122\n",
       "2         0.9722\n",
       "3         0.9781\n",
       "4         0.8479\n",
       "           ...  \n",
       "126420    0.4927\n",
       "126421    0.5709\n",
       "126422    0.9778\n",
       "126423    0.8516\n",
       "126424    0.9037\n",
       "Name: vader_compound, Length: 126425, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "notes['vader_compound'] = notes['comments'].dropna().apply(lambda t: analyzer.polarity_scores(str(t))['compound'])\n",
    "notes['vader_compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4943"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes['is_negative_review'] = notes['vader_compound'] < -0.05\n",
    "len(notes[notes['is_negative_review'] == True])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative reviews : 4,943  (3.9%)\n",
      "Positive reviews : 121,482  (96.1%)\n"
     ]
    }
   ],
   "source": [
    "neg = notes[notes['is_negative_review']].copy()\n",
    "pos = notes[~notes['is_negative_review']].copy()\n",
    "\n",
    "print(f\"Negative reviews : {len(neg):,}  ({100*len(neg)/len(notes):.1f}%)\")\n",
    "print(f\"Positive reviews : {len(pos):,}  ({100*len(pos)/len(notes):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT_LEXICONS = {\n",
    "    'cleanliness': ['clean','dirty','dust','smell','odor','hygiene','stain','mold','filth'],\n",
    "    'location':    ['location','far','remote','transport','tube','metro','walk','noise','area','neighbourhood'],\n",
    "    'value':       ['price','expensive','overpriced','cheap','worth','value','money','cost'],\n",
    "    'communication': ['host','respond','reply','communication','message','contact','rude','helpful','friendly'],\n",
    "    'checkin':     ['checkin','check-in','key','access','lock','door','arrival','late'],\n",
    "    'accuracy':    ['accurate','description','mislead','misleading','photo','picture','expect','disappoint','different'],\n",
    "}\n",
    "# LLM-ed list, to be edited later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location         1094\n",
      "communication     650\n",
      "checkin           504\n",
      "accuracy          397\n",
      "cleanliness       306\n",
      "value             154\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def tag_aspects(text):\n",
    "    text = str(text).lower()\n",
    "    return {asp: int(any(kw in text for kw in kws))\n",
    "            for asp, kws in ASPECT_LEXICONS.items()}\n",
    "\n",
    "aspect_tags = neg['comments'].apply(tag_aspects).apply(pd.Series)\n",
    "neg = pd.concat([neg.reset_index(drop=True), aspect_tags], axis=1)\n",
    "\n",
    "print(aspect_tags.sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Phase 2 ‚Äî What Are People Actually Complaining About?\n",
    "\n",
    "After scanning negative reviews for aspect keywords, here's the breakdown:\n",
    "\n",
    "| Aspect | Mentions |\n",
    "|---|---|\n",
    "| location | 1,094 |\n",
    "| communication | 650 |\n",
    "| checkin | 504 |\n",
    "| accuracy | 397 |\n",
    "| cleanliness | 306 |\n",
    "| value | 154 |\n",
    "\n",
    "**Location is #1** ‚Äî which is kind of perfect for what we're trying to prove. You can't move a flat. If someone complains about location on a listing that everyone else loves, that's on them, not the host.\n",
    "\n",
    "**Accuracy at #4** is also interesting ‚Äî \"it looked different in the photos\" is basically a mismatch complaint by definition.\n",
    "\n",
    "**Cleanliness at the bottom** makes sense too ‚Äî that one's harder to spin as a mismatch. Dirty is dirty.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install bertopic \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentence_transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install hdbscan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-02-20 12:19:00,657 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 155/155 [01:29<00:00,  1.74it/s]\n",
      "2026-02-20 12:20:29,917 - BERTopic - Embedding - Completed ‚úì\n",
      "2026-02-20 12:20:29,918 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-02-20 12:20:43,054 - BERTopic - Dimensionality - Completed ‚úì\n",
      "2026-02-20 12:20:43,055 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-02-20 12:20:43,187 - BERTopic - Cluster - Completed ‚úì\n",
      "2026-02-20 12:20:43,188 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2026-02-20 12:20:43,324 - BERTopic - Representation - Completed ‚úì\n",
      "2026-02-20 12:20:43,325 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-02-20 12:20:43,329 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-02-20 12:20:43,459 - BERTopic - Representation - Completed ‚úì\n",
      "2026-02-20 12:20:43,460 - BERTopic - Topic reduction - Reduced number of topics from 8 to 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic  Count                Name  \\\n",
      "0     -1      9     -1_br_ÎÑàÎ¨¥_Ï†ïÎßê_Í∑∏Î¶¨Í≥†   \n",
      "1      0   1935  0_und_die_sehr_ist   \n",
      "2      1   1200    1_et_de_tr√®s_est   \n",
      "3      2    717    2_the_and_to_was   \n",
      "4      3    698      3_el_la_muy_de   \n",
      "5      4    171     4_een_het_en_de   \n",
      "6      5    128  5_muito_com_de_foi   \n",
      "7      6     85  6_di_per_molto_che   \n",
      "\n",
      "                                      Representation  \\\n",
      "0    [br, ÎÑàÎ¨¥, Ï†ïÎßê, Í∑∏Î¶¨Í≥†, Îü∞Îçò, ÏûàÏäµÎãàÎã§, Îã§Ïãú, ÎïåÎ¨∏Ïóê, ÏïÑÏπ®, Ï¢ãÏïòÏäµÎãàÎã§]   \n",
      "1  [und, die, sehr, ist, war, wir, der, in, das, zu]   \n",
      "2  [et, de, tr√®s, est, nous, le, pour, la, tout, br]   \n",
      "3      [the, and, to, was, is, we, of, it, not, for]   \n",
      "4      [el, la, muy, de, en, que, con, es, no, para]   \n",
      "5       [een, het, en, de, van, je, is, og, met, op]   \n",
      "6  [muito, com, de, foi, para, em, uma, do, um, que]   \n",
      "7  [di, per, molto, che, la, il, sono, casa, con,...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [Îü∞Îçò Ïó¨ÌñâÏùÑ Í≥ÑÌöçÌïòÎ©¥ÏÑú ÎßéÏùÄ ÌõÑÍ∏∞Î•º Ï∞æÏïÑÎ≥¥Îçò ÏôÄÏ§ë Ìïú Î∏îÎ°úÍ±∞Í∞Ä Ïì¥ Í∏ÄÏùÑ Î≥¥Í≥† ÏÑ†ÌÉù...  \n",
      "1  [Ich habe ein Wochenende mit einer Freundin in...  \n",
      "2  [Nous avons pass√© 4 jours sur londres et avons...  \n",
      "3  [We stayed at Emily's flat for a week and were...  \n",
      "4  [El estudio es muy lindo, muy c√≥modo y todo fu...  \n",
      "5  [Ontvangst was hartelijk en warm. Kregen direc...  \n",
      "6  [Adoramos estar no apartamento do Costa em nos...  \n",
      "7  [La mia esperianza con Julia e' stata ottima. ...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "import re\n",
    "\n",
    "neg_clean = neg.dropna(subset=['comments'])\n",
    "\n",
    "sample = neg_clean.sample(\n",
    "    n=min(50_000, len(neg_clean)), random_state=42\n",
    ")\n",
    "docs = sample['comments'].str.strip().tolist()\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0,\n",
    "                  metric='cosine', random_state=42)\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=50, metric='euclidean',\n",
    "                         cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    nr_topics=\"auto\",          \n",
    "    calculate_probabilities=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "sample['topic'] = topics\n",
    "\n",
    "print(topic_model.get_topic_info().head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåç Phase 3 ‚Äî BERTopic Results: A Multilingual Surprise\n",
    "\n",
    "BERTopic ran successfully and found **8 topics** (including the noise bucket `-1`). But here's the thing ‚Äî it didn't find *thematic* topics at all. It found **language clusters**:\n",
    "\n",
    "| Topic | Language | Count |\n",
    "|---|---|---|\n",
    "| 0 | German (`und, die, sehr`) | 1,935 |\n",
    "| 1 | French (`et, de, tr√®s`) | 1,200 |\n",
    "| 2 | English (`the, and, to`) | 717 |\n",
    "| 3 | Spanish (`el, la, muy`) | 698 |\n",
    "| 4 | Dutch (`een, het, en`) | 171 |\n",
    "| 5 | Portuguese (`muito, com, de`) | 128 |\n",
    "| 6 | Italian (`di, per, molto`) | 85 |\n",
    "| -1 | Noise / Korean | 9 |\n",
    "\n",
    "This makes complete sense ‚Äî the London dataset is full of international tourists reviewing in their native language, and BERTopic's sentence embeddings picked up on language similarity before topic similarity.\n",
    "\n",
    "### What this means for us\n",
    "\n",
    "This isn't a failure ‚Äî it's a data quality signal. We have two options going forward:\n",
    "\n",
    "**Option A (quick)** ‚Äî filter to English-only reviews before re-running BERTopic:\n",
    "```python\n",
    "from langdetect import detect\n",
    "\n",
    "neg_clean['lang'] = neg_clean['comments'].apply(\n",
    "    lambda x: detect(str(x)) if pd.notna(x) else 'unknown'\n",
    ")\n",
    "neg_en = neg_clean[neg_clean['lang'] == 'en']\n",
    "print(f\"English reviews: {len(neg_en):,} out of {len(neg_clean):,}\")\n",
    "```\n",
    "\n",
    "**Option B (better for the paper)** ‚Äî use a multilingual embedding model like `paraphrase-multilingual-MiniLM-L12-v2` so we keep all reviews and get genuinely thematic topics across languages.\n",
    "\n",
    "> üí° Also worth noting: only **4,943 negative reviews (3.9%)** out of 126K is quite low. VADER tends to be conservative on polite hospitality text ‚Äî guests rarely write aggressively even when unhappy. Worth considering lowering the threshold from `-0.05` to `0.0`, or experimenting with a stricter rating-based definition of \"negative\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total negative reviews: 9,686\n",
      "With full score info : 4,943 (51.0%)\n",
      "\n",
      "Soft label distribution within scored negatives (%):\n",
      "review_label_soft\n",
      "ambiguous     98.81\n",
      "mismatch       1.17\n",
      "deficiency     0.02\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Mismatch share by room_type (soft):\n",
      "review_label_soft  ambiguous  deficiency  mismatch\n",
      "room_type                                         \n",
      "Entire home/apt         98.8         0.0       1.2\n",
      "Hotel room             100.0         0.0       0.0\n",
      "Private room            98.8         0.0       1.2\n",
      "Shared room            100.0         0.0       0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 0. Build neg_with_scores if it doesn't exist yet\n",
    "# (safe to re-run; it just overwrites with the correct merge)\n",
    "score_cols = [\n",
    "    'review_scores_cleanliness',\n",
    "    'review_scores_location',\n",
    "    'review_scores_value',\n",
    "    'review_scores_communication',\n",
    "    'review_scores_checkin',\n",
    "    'review_scores_accuracy',\n",
    "]\n",
    "\n",
    "aspect_cols = ['cleanliness', 'location', 'value',\n",
    "               'communication', 'checkin', 'accuracy']\n",
    "\n",
    "missing_aspects = [c for c in aspect_cols if c not in neg.columns]\n",
    "if missing_aspects:\n",
    "    raise ValueError(f\"Aspect columns missing on `neg`: {missing_aspects}\")\n",
    "\n",
    "listing_aspect_complaints = (\n",
    "    neg.groupby('listing_id')[aspect_cols]\n",
    "       .mean()\n",
    "       .rename(columns={k: f'pct_neg_{k}' for k in aspect_cols})\n",
    ")\n",
    "\n",
    "listing_scores = (\n",
    "    notes.groupby('listing_id')[score_cols]\n",
    "         .first()\n",
    ")\n",
    "\n",
    "listing_profile = listing_aspect_complaints.join(listing_scores, how='left')\n",
    "\n",
    "neg_with_scores = neg.merge(\n",
    "    listing_profile[score_cols],\n",
    "    on='listing_id',\n",
    "    how='left',\n",
    "    suffixes=('', '_listing')\n",
    ")\n",
    "\n",
    "# 1. Define the score map\n",
    "SCORE_MAP = {\n",
    "    'cleanliness':   'review_scores_cleanliness',\n",
    "    'location':      'review_scores_location',\n",
    "    'value':         'review_scores_value',\n",
    "    'communication': 'review_scores_communication',\n",
    "    'checkin':       'review_scores_checkin',\n",
    "    'accuracy':      'review_scores_accuracy',\n",
    "}\n",
    "\n",
    "# 2. Keep only negative reviews with all six scores present\n",
    "scored_cols = list(SCORE_MAP.values())\n",
    "neg_scored = neg_with_scores.dropna(subset=scored_cols).copy()\n",
    "\n",
    "print(f\"Total negative reviews: {len(neg_with_scores):,}\")\n",
    "print(f\"With full score info : {len(neg_scored):,} \"\n",
    "      f\"({len(neg_scored)/len(neg_with_scores)*100:.1f}%)\")\n",
    "\n",
    "# 3. Soft classification\n",
    "def classify_review_soft(row):\n",
    "    mismatch_signals, deficiency_signals = 0, 0\n",
    "    for asp, score_col in SCORE_MAP.items():\n",
    "        if row.get(asp, 0) == 1:  # aspect mentioned in this review\n",
    "            listing_score = row.get(score_col, np.nan)\n",
    "            if pd.isna(listing_score):\n",
    "                continue\n",
    "            if listing_score >= 4.6:\n",
    "                mismatch_signals += 1\n",
    "            elif listing_score <= 4.2:\n",
    "                deficiency_signals += 1\n",
    "    if mismatch_signals == 0 and deficiency_signals == 0:\n",
    "        return 'ambiguous'\n",
    "    return 'mismatch' if mismatch_signals >= deficiency_signals else 'deficiency'\n",
    "\n",
    "neg_scored['review_label_soft'] = neg_scored.apply(classify_review_soft, axis=1)\n",
    "\n",
    "# 4. Summary stats\n",
    "print(\"\\nSoft label distribution within scored negatives (%):\")\n",
    "print(\n",
    "    neg_scored['review_label_soft']\n",
    "        .value_counts(normalize=True)\n",
    "        .mul(100)\n",
    "        .round(2)\n",
    ")\n",
    "\n",
    "print(\"\\nMismatch share by room_type (soft):\")\n",
    "print(\n",
    "    neg_scored.groupby('room_type')['review_label_soft']\n",
    "        .value_counts(normalize=True)\n",
    "        .unstack()\n",
    "        .fillna(0)\n",
    "        .mul(100)\n",
    "        .round(1)\n",
    ")\n",
    "\n",
    "# 5. Attach soft labels back to the full neg_with_scores\n",
    "# If your review id column has a different name, adjust 'id_x' accordingly\n",
    "id_col = 'id_x' if 'id_x' in neg_scored.columns else 'id'\n",
    "neg_with_scores = neg_with_scores.merge(\n",
    "    neg_scored[[id_col, 'review_label_soft']],\n",
    "    on=id_col,\n",
    "    how='left',\n",
    "    suffixes=('', '_soft')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Phase 4 ‚Äî So‚Ä¶ why is almost everything ‚Äúambiguous‚Äù?\n",
    "\n",
    "After wiring up the score-based classifier, here‚Äôs what we see on the **4,943** negative reviews that actually have full listing scores:\n",
    "\n",
    "- **~98.8%** ‚Üí `ambiguous`\n",
    "- **~1.2%** ‚Üí `mismatch`\n",
    "- **~0.0%** ‚Üí `deficiency`\n",
    "\n",
    "At first glance this looks disappointing, but it‚Äôs actually telling us something important about the *data*, not the code:\n",
    "\n",
    "- Airbnb ratings are **crazy compressed at the top end**. Almost every listing sits somewhere between 4.2 and 4.9 on all subscores.\n",
    "- With that kind of compression, even clearly negative textual experiences do **not** translate into obviously ‚Äúlow‚Äù scores.\n",
    "- Under a strict rule like ‚Äúmismatch = high score + complaint, deficiency = low score + complaint‚Äù, almost everything will naturally fall into ‚Äúü§∑ not clearly one or the other‚Äù.\n",
    "\n",
    "So what do we take from this?\n",
    "\n",
    "- **Structured scores are too blunt an instrument** to cleanly separate ‚Äúbad fit‚Äù from ‚Äúbad quality‚Äù.\n",
    "- The ~1% of reviews that *do* pass our harsh mismatch filter are best seen as a **hard lower bound**: *at least* that many negative reviews are clear mismatches, even in a world of inflated ratings.\n",
    "- For the rest, the interesting signal is not in tiny variations between 4.6 and 4.8, but in **what people actually say**.\n",
    "\n",
    "Conclusion for the project:  \n",
    "We‚Äôll keep these labels around for sanity checks and robustness, but the real action for the research question now moves to **topics and language** (BERTopic + qualitative reading), not to slicing already-inflated numeric scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-20 12:20:48,635 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 155/155 [00:34<00:00,  4.52it/s]\n",
      "2026-02-20 12:21:22,974 - BERTopic - Embedding - Completed ‚úì\n",
      "2026-02-20 12:21:22,975 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-02-20 12:21:28,724 - BERTopic - Dimensionality - Completed ‚úì\n",
      "2026-02-20 12:21:28,725 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-02-20 12:21:28,852 - BERTopic - Cluster - Completed ‚úì\n",
      "2026-02-20 12:21:28,853 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2026-02-20 12:21:28,989 - BERTopic - Representation - Completed ‚úì\n",
      "2026-02-20 12:21:28,989 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-02-20 12:21:28,993 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-02-20 12:21:29,122 - BERTopic - Representation - Completed ‚úì\n",
      "2026-02-20 12:21:29,123 - BERTopic - Topic reduction - Reduced number of topics from 8 to 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic-level mismatch shares (soft labels, scored-only sample):\n",
      "review_label_soft  ambiguous  deficiency  mismatch  mismatch_share\n",
      "topic                                                             \n",
      " 4                       166           0         5             2.9\n",
      " 0                      1905           0        30             1.6\n",
      " 2                       708           1         8             1.1\n",
      " 3                       692           0         6             0.9\n",
      " 1                      1191           0         9             0.8\n",
      "-1                         9           0         0             0.0\n",
      " 5                       128           0         0             0.0\n",
      " 6                        85           0         0             0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if 'orig_index' not in neg.columns:\n",
    "    neg = neg.reset_index().rename(columns={'index': 'orig_index'})\n",
    "\n",
    "neg_with_scores = neg.merge(\n",
    "    listing_profile[score_cols],\n",
    "    on='listing_id',\n",
    "    how='left',\n",
    "    suffixes=('', '_listing')\n",
    ")\n",
    "\n",
    "scored_cols = list(SCORE_MAP.values())\n",
    "neg_scored = neg_with_scores.dropna(subset=scored_cols).copy()\n",
    "\n",
    "def classify_review_soft(row):\n",
    "    mismatch_signals, deficiency_signals = 0, 0\n",
    "    for asp, score_col in SCORE_MAP.items():\n",
    "        if row.get(asp, 0) == 1:\n",
    "            listing_score = row.get(score_col, np.nan)\n",
    "            if pd.isna(listing_score):\n",
    "                continue\n",
    "            if listing_score >= 4.6:\n",
    "                mismatch_signals += 1\n",
    "            elif listing_score <= 4.2:\n",
    "                deficiency_signals += 1\n",
    "    if mismatch_signals == 0 and deficiency_signals == 0:\n",
    "        return 'ambiguous'\n",
    "    return 'mismatch' if mismatch_signals >= deficiency_signals else 'deficiency'\n",
    "\n",
    "neg_scored['review_label_soft'] = neg_scored.apply(classify_review_soft, axis=1)\n",
    "\n",
    "\n",
    "neg_scored_clean = neg_scored.dropna(subset=['comments'])\n",
    "sample = neg_scored_clean.sample(\n",
    "    n=min(10_000, len(neg_scored_clean)), random_state=42\n",
    ").copy()\n",
    "\n",
    "docs = sample['comments'].str.strip().tolist()\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=15, n_components=5, min_dist=0.0,\n",
    "    metric='cosine', random_state=42\n",
    ")\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=50, metric='euclidean',\n",
    "    cluster_selection_method='eom', prediction_data=True\n",
    ")\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    nr_topics=\"auto\",\n",
    "    calculate_probabilities=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "sample['topic'] = topics\n",
    "\n",
    "assert 'review_label_soft' in sample.columns\n",
    "\n",
    "topic_cross = (\n",
    "    sample.groupby(['topic', 'review_label_soft'])\n",
    "          .size()\n",
    "          .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "topic_cross['mismatch_share'] = (\n",
    "    topic_cross.get('mismatch', 0) /\n",
    "    topic_cross.sum(axis=1) * 100\n",
    ").round(1)\n",
    "\n",
    "print(\"\\nTopic-level mismatch shares (soft labels, scored-only sample):\")\n",
    "print(topic_cross.sort_values('mismatch_share', ascending=False).head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Phase 5 ‚Äî Topics vs. Mismatches (a tiny but real signal)\n",
    "\n",
    "On our scored negative sample, BERTopic found **8 topics**.  \n",
    "When we overlay the soft mismatch labels, the picture is:\n",
    "\n",
    "| Topic | ambiguous | mismatch | mismatch share |\n",
    "|-------|-----------|----------|----------------|\n",
    "| 4     | 166       | 5        | 2.9%           |\n",
    "| 0     | 1905      | 30       | 1.6%           |\n",
    "| 2     | 708       | 8        | 1.1%           |\n",
    "| 3     | 692       | 6        | 0.9%           |\n",
    "| 1     | 1191      | 9        | 0.8%           |\n",
    "| 5/6/-1| 100% amb. | 0        | 0.0%           |\n",
    "\n",
    "Given how ratings are squashed near 5 stars, **any** mismatch signal is already impressive. A few things stand out:\n",
    "\n",
    "- Topic **4** clearly has the **highest mismatch share (~3%)**. This is our best candidate for a ‚Äúclassic mismatch‚Äù theme: guests complaining even though the listing‚Äôs scores look great.\n",
    "- Topics **0‚Äì3** also contain some mismatches, but at a lower rate (~1‚Äì1.6%). They‚Äôre more mixed bags: mostly ‚Äúgeneric‚Äù negatives with a small mismatch tail.\n",
    "- Topics **5, 6, and the noise bucket (-1)** are basically all ambiguous under our rules, so they‚Äôre not helpful for score-based separation.\n",
    "\n",
    "The key takeaway is not the exact percentages (they‚Äôre tiny by construction), but **which topics systematically over-index on mismatches**. Those are the clusters we want to read closely and describe qualitatively as ‚Äúlatent interest vs. listing affordance‚Äù failures in the write‚Äëup.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
